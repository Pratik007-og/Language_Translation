{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM,Dense,Input\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dims = 256\n",
    "num_samples = 10000\n",
    "\n",
    "data_path = 'fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path,'r',encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text,target_text,_ = line.split(\"\\t\")\n",
    "    \n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "        \n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\xa0',\n",
       " '«',\n",
       " '»',\n",
       " 'À',\n",
       " 'Ç',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'à',\n",
       " 'â',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ô',\n",
       " 'ù',\n",
       " 'û',\n",
       " 'œ',\n",
       " '\\u2009',\n",
       " '’',\n",
       " '\\u202f'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_len = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_len = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input samples =  10000\n",
      "Number of unique input tokens =  71\n",
      "Number of unique output tokens =  92\n",
      "Maximum sequence length for input =  15\n",
      "Maximum sequence length for output =  59\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of input samples = \", len(input_texts))\n",
    "print(\"Number of unique input tokens = \",num_encoder_tokens)\n",
    "print(\"Number of unique output tokens = \",num_decoder_tokens)\n",
    "print(\"Maximum sequence length for input = \",max_encoder_seq_len)\n",
    "print(\"Maximum sequence length for output = \",max_decoder_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char,i) for i,char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char,i) for i,char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index['K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts),max_encoder_seq_len,num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data = np.zeros((len(target_texts),max_decoder_seq_len,num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts),max_decoder_seq_len,num_decoder_tokens),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for t,char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]] = 1.\n",
    "    encoder_input_data[i,t+1:,input_token_index[' ']] = 1.\n",
    "    \n",
    "    for t,char in enumerate(target_text):\n",
    "        decoder_input_data[i,t,target_token_index[char]] = 1.\n",
    "    \n",
    "        if t>0:\n",
    "            decoder_target_data[i,t-1,target_token_index[char]] = 1.\n",
    "    decoder_input_data[i,t+1:,target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i,t:,target_token_index[' ']] = 1.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 71)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 92)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape = (None,num_encoder_tokens))\n",
    "encoder = LSTM(latent_dims,return_state=True)\n",
    "encoder_outputs,state_h,state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape = (None,num_decoder_tokens))\n",
    "decoder = LSTM(latent_dims,return_sequences = True,return_state = True )\n",
    "decoder_outputs,_,_ = decoder(decoder_inputs,initial_state = encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens,activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 31s 212ms/step - loss: 1.1282 - val_loss: 1.0108\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.8094 - val_loss: 0.8260\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 23s 180ms/step - loss: 0.6547 - val_loss: 0.6974\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.5747 - val_loss: 0.6325\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 22s 178ms/step - loss: 0.5296 - val_loss: 0.5990\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.4944 - val_loss: 0.5630\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.4657 - val_loss: 0.5364\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.4416 - val_loss: 0.5236\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.4204 - val_loss: 0.5038\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 33s 264ms/step - loss: 0.4012 - val_loss: 0.4966\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.3842 - val_loss: 0.4862\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.3684 - val_loss: 0.4721\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.3536 - val_loss: 0.4653\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.3397 - val_loss: 0.4612\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.3260 - val_loss: 0.4583\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.3140 - val_loss: 0.4511\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.3020 - val_loss: 0.4509\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.2908 - val_loss: 0.4477\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.2801 - val_loss: 0.4486\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.2698 - val_loss: 0.4467\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.2603 - val_loss: 0.4464\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.2510 - val_loss: 0.4442\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.2422 - val_loss: 0.4459\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.2342 - val_loss: 0.4547\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.2266 - val_loss: 0.4469\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.2186 - val_loss: 0.4535\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 27s 212ms/step - loss: 0.2113 - val_loss: 0.4535\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.2047 - val_loss: 0.4583\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 27s 219ms/step - loss: 0.1977 - val_loss: 0.4571\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.1913 - val_loss: 0.4633\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.1853 - val_loss: 0.4697\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1795 - val_loss: 0.4755\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1739 - val_loss: 0.4816\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1689 - val_loss: 0.4813\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1636 - val_loss: 0.4827\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1589 - val_loss: 0.4939\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.1543 - val_loss: 0.4937\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.1497 - val_loss: 0.4992\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.1455 - val_loss: 0.5064\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.1415 - val_loss: 0.5092 l\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.1374 - val_loss: 0.5130\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 25s 196ms/step - loss: 0.1338 - val_loss: 0.5217\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 25s 196ms/step - loss: 0.1302 - val_loss: 0.5245\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.1269 - val_loss: 0.5256\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 25s 196ms/step - loss: 0.1232 - val_loss: 0.5339\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.1201 - val_loss: 0.5406\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.1173 - val_loss: 0.5495\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1147 - val_loss: 0.5493\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.1115 - val_loss: 0.5529\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.1089 - val_loss: 0.5593\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.1062 - val_loss: 0.5653\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.1038 - val_loss: 0.5693\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.1011 - val_loss: 0.5736\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.0990 - val_loss: 0.5780\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.0971 - val_loss: 0.5824\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.0948 - val_loss: 0.5882\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.0927 - val_loss: 0.5901\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.0905 - val_loss: 0.5948\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.0886 - val_loss: 0.6029\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.0867 - val_loss: 0.6083\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.0853 - val_loss: 0.6139\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.0828 - val_loss: 0.6171\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 32s 258ms/step - loss: 0.0820 - val_loss: 0.6320\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.0802 - val_loss: 0.6241\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 31s 252ms/step - loss: 0.0788 - val_loss: 0.6285\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 32s 253ms/step - loss: 0.0772 - val_loss: 0.6391\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 31s 244ms/step - loss: 0.0754 - val_loss: 0.6437\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 30s 243ms/step - loss: 0.0735 - val_loss: 0.6478\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 32s 254ms/step - loss: 0.0729 - val_loss: 0.6575\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 30s 239ms/step - loss: 0.0717 - val_loss: 0.6543\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.0701 - val_loss: 0.6575\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.0693 - val_loss: 0.6589\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 29s 236ms/step - loss: 0.0676 - val_loss: 0.6655\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 29s 236ms/step - loss: 0.0669 - val_loss: 0.6740\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 29s 235ms/step - loss: 0.0656 - val_loss: 0.6718\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.0644 - val_loss: 0.6752\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 29s 228ms/step - loss: 0.0633 - val_loss: 0.6780\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.0621 - val_loss: 0.6841\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 30s 237ms/step - loss: 0.0612 - val_loss: 0.6849\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 29s 236ms/step - loss: 0.0604 - val_loss: 0.6881\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 30s 244ms/step - loss: 0.0593 - val_loss: 0.6913\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.0581 - val_loss: 0.6974\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 30s 243ms/step - loss: 0.0574 - val_loss: 0.7098\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.0565 - val_loss: 0.7087\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 28s 220ms/step - loss: 0.0559 - val_loss: 0.7112\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 0.0546 - val_loss: 0.7141\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 27s 218ms/step - loss: 0.0541 - val_loss: 0.7161\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.0534 - val_loss: 0.7183\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.0526 - val_loss: 0.7297\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.0517 - val_loss: 0.7268\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.0511 - val_loss: 0.7226\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.0505 - val_loss: 0.7397\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.0498 - val_loss: 0.7360\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.0493 - val_loss: 0.7464\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0487 - val_loss: 0.7419\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.0500 - val_loss: 0.7429\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0465 - val_loss: 0.7511\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0472 - val_loss: 0.7498\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0466 - val_loss: 0.7518\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 0.0459 - val_loss: 0.7621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x294084185e0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
    "model.fit([encoder_input_data,decoder_input_data], decoder_target_data,\n",
    "         batch_size = batch_size,\n",
    "         epochs = epochs,\n",
    "         validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input Sequence:  Go.\n",
      "Decoded Sequence:  Va !\n",
      "\n",
      "-\n",
      "Input Sequence:  Go.\n",
      "Decoded Sequence:  Va !\n",
      "\n",
      "-\n",
      "Input Sequence:  Go.\n",
      "Decoded Sequence:  Va !\n",
      "\n",
      "-\n",
      "Input Sequence:  Hi.\n",
      "Decoded Sequence:  Salut.\n",
      "\n",
      "-\n",
      "Input Sequence:  Hi.\n",
      "Decoded Sequence:  Salut.\n",
      "\n",
      "-\n",
      "Input Sequence:  Run!\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run!\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run!\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run!\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run!\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run!\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run!\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run!\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run.\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run.\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run.\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run.\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run.\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run.\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run.\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Run.\n",
      "Decoded Sequence:  Fuyez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Who?\n",
      "Decoded Sequence:  Qui ?\n",
      "\n",
      "-\n",
      "Input Sequence:  Wow!\n",
      "Decoded Sequence:  Ça alore !\n",
      "\n",
      "-\n",
      "Input Sequence:  Fire!\n",
      "Decoded Sequence:  Au feu !\n",
      "\n",
      "-\n",
      "Input Sequence:  Help!\n",
      "Decoded Sequence:  À l'aide !\n",
      "\n",
      "-\n",
      "Input Sequence:  Hide.\n",
      "Decoded Sequence:  Cache-toi.\n",
      "\n",
      "-\n",
      "Input Sequence:  Hide.\n",
      "Decoded Sequence:  Cache-toi.\n",
      "\n",
      "-\n",
      "Input Sequence:  Jump!\n",
      "Decoded Sequence:  Saute.\n",
      "\n",
      "-\n",
      "Input Sequence:  Jump.\n",
      "Decoded Sequence:  Saute.\n",
      "\n",
      "-\n",
      "Input Sequence:  Stop!\n",
      "Decoded Sequence:  Ça suffit !\n",
      "\n",
      "-\n",
      "Input Sequence:  Stop!\n",
      "Decoded Sequence:  Ça suffit !\n",
      "\n",
      "-\n",
      "Input Sequence:  Stop!\n",
      "Decoded Sequence:  Ça suffit !\n",
      "\n",
      "-\n",
      "Input Sequence:  Wait!\n",
      "Decoded Sequence:  Attendez.\n",
      "\n",
      "-\n",
      "Input Sequence:  Wait!\n",
      "Decoded Sequence:  Attendez.\n",
      "\n",
      "-\n",
      "Input Sequence:  Wait!\n",
      "Decoded Sequence:  Attendez.\n",
      "\n",
      "-\n",
      "Input Sequence:  Wait.\n",
      "Decoded Sequence:  Attendez.\n",
      "\n",
      "-\n",
      "Input Sequence:  Wait.\n",
      "Decoded Sequence:  Attendez.\n",
      "\n",
      "-\n",
      "Input Sequence:  Wait.\n",
      "Decoded Sequence:  Attendez.\n",
      "\n",
      "-\n",
      "Input Sequence:  Wait.\n",
      "Decoded Sequence:  Attendez.\n",
      "\n",
      "-\n",
      "Input Sequence:  Begin.\n",
      "Decoded Sequence:  Commence.\n",
      "\n",
      "-\n",
      "Input Sequence:  Begin.\n",
      "Decoded Sequence:  Commence.\n",
      "\n",
      "-\n",
      "Input Sequence:  Go on.\n",
      "Decoded Sequence:  Poursuis.\n",
      "\n",
      "-\n",
      "Input Sequence:  Go on.\n",
      "Decoded Sequence:  Poursuis.\n",
      "\n",
      "-\n",
      "Input Sequence:  Go on.\n",
      "Decoded Sequence:  Poursuis.\n",
      "\n",
      "-\n",
      "Input Sequence:  Hello!\n",
      "Decoded Sequence:  Salut !\n",
      "\n",
      "-\n",
      "Input Sequence:  Hello!\n",
      "Decoded Sequence:  Salut !\n",
      "\n",
      "-\n",
      "Input Sequence:  I see.\n",
      "Decoded Sequence:  Je comprends.\n",
      "\n",
      "-\n",
      "Input Sequence:  I see.\n",
      "Decoded Sequence:  Je comprends.\n",
      "\n",
      "-\n",
      "Input Sequence:  I try.\n",
      "Decoded Sequence:  J'essaye.\n",
      "\n",
      "-\n",
      "Input Sequence:  I won!\n",
      "Decoded Sequence:  Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input Sequence:  I won!\n",
      "Decoded Sequence:  Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input Sequence:  I won.\n",
      "Decoded Sequence:  Je sais lerfiane.\n",
      "\n",
      "-\n",
      "Input Sequence:  Oh no!\n",
      "Decoded Sequence:  Oh non !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Relax.\n",
      "Decoded Sequence:  Détends-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Smile.\n",
      "Decoded Sequence:  Souriez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Smile.\n",
      "Decoded Sequence:  Souriez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Smile.\n",
      "Decoded Sequence:  Souriez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Sorry?\n",
      "Decoded Sequence:  Pardon ?\n",
      "\n",
      "-\n",
      "Input Sequence:  Attack!\n",
      "Decoded Sequence:  Attaquez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Attack!\n",
      "Decoded Sequence:  Attaquez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Attack!\n",
      "Decoded Sequence:  Attaquez !\n",
      "\n",
      "-\n",
      "Input Sequence:  Buy it.\n",
      "Decoded Sequence:  Achetez-la !\n",
      "\n",
      "-\n",
      "Input Sequence:  Buy it.\n",
      "Decoded Sequence:  Achetez-la !\n",
      "\n",
      "-\n",
      "Input Sequence:  Buy it.\n",
      "Decoded Sequence:  Achetez-la !\n",
      "\n",
      "-\n",
      "Input Sequence:  Buy it.\n",
      "Decoded Sequence:  Achetez-la !\n",
      "\n",
      "-\n",
      "Input Sequence:  Cheers!\n",
      "Decoded Sequence:  À votre santé !\n",
      "\n",
      "-\n",
      "Input Sequence:  Cheers!\n",
      "Decoded Sequence:  À votre santé !\n",
      "\n",
      "-\n",
      "Input Sequence:  Cheers!\n",
      "Decoded Sequence:  À votre santé !\n",
      "\n",
      "-\n",
      "Input Sequence:  Cheers!\n",
      "Decoded Sequence:  À votre santé !\n",
      "\n",
      "-\n",
      "Input Sequence:  Eat it.\n",
      "Decoded Sequence:  Mange-le.\n",
      "\n",
      "-\n",
      "Input Sequence:  Eat it.\n",
      "Decoded Sequence:  Mange-le.\n",
      "\n",
      "-\n",
      "Input Sequence:  Get up.\n",
      "Decoded Sequence:  Lève-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Get up.\n",
      "Decoded Sequence:  Lève-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Get up.\n",
      "Decoded Sequence:  Lève-toi !\n",
      "\n",
      "-\n",
      "Input Sequence:  Go now.\n",
      "Decoded Sequence:  Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input Sequence:  Go now.\n",
      "Decoded Sequence:  Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input Sequence:  Go now.\n",
      "Decoded Sequence:  Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input Sequence:  Got it!\n",
      "Decoded Sequence:  Aha !\n",
      "\n",
      "-\n",
      "Input Sequence:  Got it!\n",
      "Decoded Sequence:  Aha !\n",
      "\n",
      "-\n",
      "Input Sequence:  Got it!\n",
      "Decoded Sequence:  Aha !\n",
      "\n",
      "-\n",
      "Input Sequence:  Got it?\n",
      "Decoded Sequence:  T'as capté ?\n",
      "\n",
      "-\n",
      "Input Sequence:  Got it?\n",
      "Decoded Sequence:  T'as capté ?\n",
      "\n",
      "-\n",
      "Input Sequence:  Got it?\n",
      "Decoded Sequence:  T'as capté ?\n",
      "\n",
      "-\n",
      "Input Sequence:  Hop in.\n",
      "Decoded Sequence:  Monte.\n",
      "\n",
      "-\n",
      "Input Sequence:  Hop in.\n",
      "Decoded Sequence:  Monte.\n",
      "\n",
      "-\n",
      "Input Sequence:  Hug me.\n",
      "Decoded Sequence:  Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input Sequence:  Hug me.\n",
      "Decoded Sequence:  Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input Sequence:  I fell.\n",
      "Decoded Sequence:  Je suis tombée.\n",
      "\n",
      "-\n",
      "Input Sequence:  I fell.\n",
      "Decoded Sequence:  Je suis tombée.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "\n",
    "decoder_state_h = Input(shape = (latent_dims,))\n",
    "decoder_state_c = Input(shape = (latent_dims,))\n",
    "decoder_states_inputs = [decoder_state_h,decoder_state_c]\n",
    "decoder_outputs,state_h,state_c = decoder(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h,state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict((i,char) for char,i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i,char) for char,i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print('-')\n",
    "    print(\"Input Sequence: \",input_texts[seq_index])\n",
    "    print(\"Decoded Sequence: \",decoded_sentence)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
